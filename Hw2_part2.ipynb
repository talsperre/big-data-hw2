{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2433c2c5-8ed8-471b-9758-4e06e04874da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchinfo as torchinfo\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b0608c-4901-41ee-93e9-98896121e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952fb068-d2be-44ab-ae30-60c4096a5fa1",
   "metadata": {},
   "source": [
    "### Test on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fb1e926-817f-43d9-965e-a204180f422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomCrop(480),\n",
    "    transforms.Resize(28),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28604c00-da96-4e73-8eaf-8038ee0f6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.imgs = sorted([img for img in os.listdir(self.root_dir) if img.endswith('.jpeg')])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.imgs[idx])\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        img = img.unsqueeze(0)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fb10023-c3f9-4f3e-aefd-e98c2719c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_dataset = OutlierDataset(\"./data/images/\", transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126f938-4a14-4ba8-9e01-138fd5b65a07",
   "metadata": {},
   "source": [
    "#### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d313ee6-e08f-45ab-92e7-bebcb9bafdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, inp_channels=1, num_classes=10, batch_norm=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.batch_norm = batch_norm\n",
    "        self.conv1 = nn.Conv2d(inp_channels, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.batch_norm:\n",
    "            x = self.activation(self.bn1(self.conv1(x)))\n",
    "            x = self.pool1(self.activation(self.bn2(self.conv2(x))))\n",
    "        else:\n",
    "            x = self.activation(self.conv1(x))\n",
    "            x = self.pool1(self.activation(self.conv2(x)))\n",
    "        x = self.dropout1(x).flatten(1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(self.dropout2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0eefc15-d82d-4012-821e-74b40203f1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load(\"./models/default.pt\"))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdfae69-f376-41ee-9007-367f147143a2",
   "metadata": {},
   "source": [
    "### Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48ae5c5e-2d11-4613-b52f-c81ad429e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "img_list = []\n",
    "for i, img in enumerate(outlier_dataset):\n",
    "    img = img.to(device)\n",
    "    out = model(img)\n",
    "    pred = torch.argmax(out, dim=1)\n",
    "    pred_list.append(pred.item())\n",
    "    img_list.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "18b88050-7cb8-43ae-95eb-9e8ac3b74593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [7, 6, 6, 2, 7, 7, 1, 2, 2, 7]\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels: {}\".format(pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0433273b-860e-4176-b66c-5ba1342015f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.cat(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "03389dbb-db11-4cce-8d5e-7694bac7f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img * 0.3081 + 0.1307\n",
    "    npimg = img.numpy().squeeze()\n",
    "    \n",
    "    plt.imshow(np.uint8(npimg * 255.0), cmap='gray', vmin=0, vmax=255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84130c-24fe-4bf4-ada7-3c5fd0533779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
